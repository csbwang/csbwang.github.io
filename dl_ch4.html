<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="warmber" />
        <meta name="copyright" content="warmber" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="machine learning, 深度学习500问, " />

<meta property="og:title" content="第四章 经典网络解读 "/>
<meta property="og:url" content="https://csbwang.github.io/dl_ch4" />
<meta property="og:description" content="" />
<meta property="og:site_name" content="warmber&#39;s Blog" />
<meta property="og:article:author" content="warmber" />
<meta property="og:article:published_time" content="2019-07-06T00:00:00+08:00" />
<meta property="" content="2019-07-06T00:00:00+08:00" />
<meta name="twitter:title" content="第四章 经典网络解读 ">
<meta name="twitter:description" content="">

        <title>第四章 经典网络解读  · warmber&#39;s Blog
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="https://csbwang.github.io/theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://csbwang.github.io/theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://csbwang.github.io/theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://csbwang.github.io/theme/css/custom.css" media="screen">
        <link rel="shortcut icon" href="https://csbwang.github.io/theme/images/favicon.ico" type="image/x-icon" type="image/png" />
        <link rel="icon" href="https://csbwang.github.io/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://csbwang.github.io/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://csbwang.github.io/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://csbwang.github.io/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://csbwang.github.io/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://csbwang.github.io/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://csbwang.github.io/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://csbwang.github.io/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://csbwang.github.io/theme/images/apple-touch-icon-152x152.png" type="image/png" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="https://csbwang.github.io/"><span class=site-name>warmber's Blog</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="https://csbwang.github.io">Home</a></li>
                            <li ><a href="https://csbwang.github.io/categories.html">Categories</a></li>
                            <li ><a href="https://csbwang.github.io/tags.html">Tags</a></li>
                            <li ><a href="https://csbwang.github.io/archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="https://csbwang.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="https://csbwang.github.io/dl_ch4"> 第四章&nbsp;经典网络解读  </a></h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#_1">第四章 经典网络解读</a><ul>
<li><a href="#41-lenet-5">4.1 LeNet-5</a><ul>
<li><a href="#411">4.1.1 模型介绍</a></li>
<li><a href="#412">4.1.2 模型结构</a></li>
<li><a href="#413">4.1.3 模型特性</a></li>
</ul>
</li>
<li><a href="#42-alexnet">4.2 AlexNet</a><ul>
<li><a href="#421">4.2.1 模型介绍</a></li>
<li><a href="#422">4.2.2 模型结构</a></li>
<li><a href="#423">4.2.3 模型特性</a></li>
</ul>
</li>
<li><a href="#43-zfnet">4.3 ZFNet</a><ul>
<li><a href="#431">4.3.1 模型介绍</a></li>
<li><a href="#432">4.3.2 模型结构</a></li>
<li><a href="#433">4.3.3 模型特性</a></li>
</ul>
</li>
<li><a href="#44-network-in-network">4.4 Network in Network</a><ul>
<li><a href="#441">4.4.1 模型介绍</a></li>
<li><a href="#442">4.4.2 模型结构</a></li>
<li><a href="#443">4.4.3 模型特点</a></li>
</ul>
</li>
<li><a href="#45-vggnet">4.5 VGGNet</a><ul>
<li><a href="#451">4.5.1 模型介绍</a></li>
<li><a href="#452">4.5.2 模型结构</a></li>
<li><a href="#453">4.5.3 模型特性</a></li>
</ul>
</li>
<li><a href="#46-googlenet">4.6 GoogLeNet</a><ul>
<li><a href="#461">4.6.1 模型介绍</a></li>
<li><a href="#462">4.6.2 模型结构</a></li>
<li><a href="#463">4.6.3 模型特性</a></li>
</ul>
</li>
<li><a href="#restnet">Restnet</a></li>
<li><a href="#densenet">Densenet</a></li>
<li><a href="#47-cnngooglenetvggnetalexnet">4.7 为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？</a></li>
<li><a href="#_2">参考文献</a></li>
</ul>
</li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">

            
            
<h1 id="_1">第四章 经典网络解读<a class="headerlink" href="#_1" title="Permanent link">¶</a></h1>
<h2 id="41-lenet-5">4.1 LeNet-5<a class="headerlink" href="#41-lenet-5" title="Permanent link">¶</a></h2>
<h3 id="411">4.1.1 模型介绍<a class="headerlink" href="#411" title="Permanent link">¶</a></h3>
<p>​   LeNet-5是由<span class="math">\(LeCun\)</span> 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络（Convolutional Neural Network，<span class="caps">CNN</span>）<span class="math">\(^{[1]}\)</span>，其命名来源于作者<span class="math">\(LeCun\)</span>的名字，5则是其研究成果的代号，在LeNet-5之前还有LeNet-4和LeNet-1鲜为人知。LeNet-5阐述了图像中像素特征之间的相关性能够由参数共享的卷积操作所提取，同时使用卷积、下采样（池化）和非线性映射这样的组合结构，是当前流行的大多数深度图像识别网络的基础。</p>
<h3 id="412">4.1.2 模型结构<a class="headerlink" href="#412" title="Permanent link">¶</a></h3>
<p><img alt="" src="./img/ch4/image1.png"/></p>
<p>​                                                                 图4.1 LeNet-5网络结构图</p>
<p>​   如图4.1所示，LeNet-5一共包含7层（输入层不作为网络结构），分别由2个卷积层、2个下采样层和3个连接层组成，网络的参数配置如表4.1所示，其中下采样层和全连接层的核尺寸分别代表采样范围和连接矩阵的尺寸（如卷积核尺寸中的<span class="math">\(“5\times5\times1/1,6”\)</span>表示核大小为<span class="math">\(5\times5\times1\)</span>、步长为<span class="math">\(1​\)</span>且核个数为6的卷积核）。</p>
<p>​                                                                 表4.1 LeNet-5网络参数配置</p>
<table>
<thead>
<tr>
<th>网络层</th>
<th>输入尺寸</th>
<th>核尺寸</th>
<th>输出尺寸</th>
<th>可训练参数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>卷积层<span class="math">\(C_1\)</span></td>
<td><span class="math">\(32\times32\times1\)</span></td>
<td><span class="math">\(5\times5\times1/1,6\)</span></td>
<td><span class="math">\(28\times28\times6\)</span></td>
<td><span class="math">\((5\times5\times1+1)\times6\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_2\)</span></td>
<td><span class="math">\(28\times28\times6\)</span></td>
<td><span class="math">\(2\times2/2\)</span></td>
<td><span class="math">\(14\times14\times6\)</span></td>
<td><span class="math">\((1+1)\times6\)</span> <span class="math">\(^*\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_3\)</span></td>
<td><span class="math">\(14\times14\times6\)</span></td>
<td><span class="math">\(5\times5\times6/1,16\)</span></td>
<td><span class="math">\(10\times10\times16\)</span></td>
<td><span class="math">\(1516^*\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_4\)</span></td>
<td><span class="math">\(10\times10\times16\)</span></td>
<td><span class="math">\(2\times2/2\)</span></td>
<td><span class="math">\(5\times5\times16\)</span></td>
<td><span class="math">\((1+1)\times16\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_5\)</span><span class="math">\(^*\)</span></td>
<td><span class="math">\(5\times5\times16\)</span></td>
<td><span class="math">\(5\times5\times16/1,120\)</span></td>
<td><span class="math">\(1\times1\times120\)</span></td>
<td><span class="math">\((5\times5\times16+1)\times120\)</span></td>
</tr>
<tr>
<td>全连接层<span class="math">\(F_6\)</span></td>
<td><span class="math">\(1\times1\times120\)</span></td>
<td><span class="math">\(120\times84\)</span></td>
<td><span class="math">\(1\times1\times84\)</span></td>
<td><span class="math">\((120+1)\times84\)</span></td>
</tr>
<tr>
<td>输出层</td>
<td><span class="math">\(1\times1\times84\)</span></td>
<td><span class="math">\(84\times10\)</span></td>
<td><span class="math">\(1\times1\times10\)</span></td>
<td><span class="math">\((84+1)\times10\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>​ <span class="math">\(^*\)</span> 在LeNet中，下采样操作和池化操作类似，但是在得到采样结果后会乘以一个系数和加上一个偏置项，所以下采样的参数个数是<span class="math">\((1+1)\times6​\)</span>而不是零。</p>
<p>​ <span class="math">\(^*\)</span> <span class="math">\(C_3\)</span>卷积层可训练参数并未直接连接<span class="math">\(S_2\)</span>中所有的特征图（Feature Map），而是采用如图4.2所示的采样特征方式进行连接（稀疏连接），生成的16个通道特征图中分别按照相邻3个特征图、相邻4个特征图、非相邻4个特征图和全部6个特征图进行映射，得到的参数个数计算公式为<span class="math">\(6\times(25\times3+1)+6\times(25\times4+1)+3\times(25\times4+1)+1\times(25\times6+1)=1516\)</span>，在原论文中解释了使用这种采样方式原因包含两点：限制了连接数不至于过大（当年的计算能力比较弱）;强制限定不同特征图的组合可以使映射得到的特征图学习到不同的特征模式。</p>
</blockquote>
<p><img alt="FeatureMap" src="./img/ch4/featureMap.jpg"/></p>
<p>​                                                                图4.2 <span class="math">\(S_2\)</span>与<span class="math">\(C_3\)</span>之间的特征图稀疏连接</p>
<blockquote>
<p>​ <span class="math">\(^*\)</span> <span class="math">\(C_5\)</span>卷积层在图4.1中显示为全连接层，原论文中解释这里实际采用的是卷积操作，只是刚好在<span class="math">\(5\times5\)</span>卷积后尺寸被压缩为<span class="math">\(1\times1​\)</span>，输出结果看起来和全连接很相似。</p>
</blockquote>
<h3 id="413">4.1.3 模型特性<a class="headerlink" href="#413" title="Permanent link">¶</a></h3>
<ul>
<li>卷积网络使用一个3层的序列组合：卷积、下采样（池化）、非线性映射（LeNet-5最重要的特性，奠定了目前深层卷积网络的基础）</li>
<li>使用卷积提取空间特征</li>
<li>使用映射的空间均值进行下采样</li>
<li>使用<span class="math">\(tanh\)</span>或<span class="math">\(sigmoid\)</span>进行非线性映射</li>
<li>多层神经网络（<span class="caps">MLP</span>）作为最终的分类器</li>
<li>层间的稀疏连接矩阵以避免巨大的计算开销</li>
</ul>
<h2 id="42-alexnet">4.2 AlexNet<a class="headerlink" href="#42-alexnet" title="Permanent link">¶</a></h2>
<h3 id="421">4.2.1 模型介绍<a class="headerlink" href="#421" title="Permanent link">¶</a></h3>
<p>​   AlexNet是由<span class="math">\(Alex\)</span> <span class="math">\(Krizhevsky $提出的首个应用于图像分类的深层卷积神经网络，该网络在2012年ILSVRC（ImageNet Large Scale Visual Recognition Competition）图像分类竞赛中以15.3%的top-5测试错误率赢得第一名\)</span>^{[2]}$。AlexNet使用GPU代替CPU进行运算，使得在可接受的时间范围内模型结构能够更加复杂，它的出现证明了深层卷积神经网络在复杂模型下的有效性，使CNN在计算机视觉中流行开来，直接或间接地引发了深度学习的热潮。</p>
<h3 id="422">4.2.2 模型结构<a class="headerlink" href="#422" title="Permanent link">¶</a></h3>
<p><img alt="" src="./img/ch4/alexnet.png"/></p>
<p>​                                                                         图4.3 AlexNet网络结构图</p>
<p>​   如图4.3所示，除去下采样（池化层）和局部响应规范化操作（Local Responsible Normalization, <span class="caps">LRN</span>），AlexNet一共包含8层，前5层由卷积层组成，而剩下的3层为全连接层。网络结构分为上下两层，分别对应两个GPU的操作过程，除了中间某些层（<span class="math">\(C_3\)</span>卷积层和<span class="math">\(F_{6-8}\)</span>全连接层会有GPU间的交互），其他层两个GPU分别计算结 果。最后一层全连接层的输出作为<span class="math">\(softmax\)</span>的输入，得到1000个图像分类标签对应的概率值。除去GPU并行结构的设计，AlexNet网络结构与LeNet十分相似，其网络的参数配置如表4.2所示。</p>
<p>​                                   表4.2 AlexNet网络参数配置</p>
<table>
<thead>
<tr>
<th>网络层</th>
<th>输入尺寸</th>
<th>核尺寸</th>
<th>输出尺寸</th>
<th>可训练参数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>卷积层<span class="math">\(C_1\)</span> <span class="math">\(^*\)</span></td>
<td><span class="math">\(224\times224\times3\)</span></td>
<td><span class="math">\(11\times11\times3/4,48(\times2_{GPU})\)</span></td>
<td><span class="math">\(55\times55\times48(\times2_{GPU})\)</span></td>
<td><span class="math">\((11\times11\times3+1)\times48\times2\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max}\)</span><span class="math">\(^*\)</span></td>
<td><span class="math">\(55\times55\times48(\times2_{GPU})\)</span></td>
<td><span class="math">\(3\times3/2(\times2_{GPU})\)</span></td>
<td><span class="math">\(27\times27\times48(\times2_{GPU})\)</span></td>
<td>0</td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_2\)</span></td>
<td><span class="math">\(27\times27\times48(\times2_{GPU})\)</span></td>
<td><span class="math">\(5\times5\times48/1,128(\times2_{GPU})\)</span></td>
<td><span class="math">\(27\times27\times128(\times2_{GPU})\)</span></td>
<td><span class="math">\((5\times5\times48+1)\times128\times2\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max}\)</span></td>
<td><span class="math">\(27\times27\times128(\times2_{GPU})\)</span></td>
<td><span class="math">\(3\times3/2(\times2_{GPU})\)</span></td>
<td><span class="math">\(13\times13\times128(\times2_{GPU})\)</span></td>
<td>0</td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_3\)</span> <span class="math">\(^*\)</span></td>
<td><span class="math">\(13\times13\times128\times2_{GPU}\)</span></td>
<td><span class="math">\(3\times3\times256/1,192(\times2_{GPU})\)</span></td>
<td><span class="math">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td><span class="math">\((3\times3\times256+1)\times192\times2\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_4\)</span></td>
<td><span class="math">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td><span class="math">\(3\times3\times192/1,192(\times2_{GPU})\)</span></td>
<td><span class="math">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td><span class="math">\((3\times3\times192+1)\times192\times2\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_5\)</span></td>
<td><span class="math">\(13\times13\times192(\times2_{GPU})\)</span></td>
<td><span class="math">\(3\times3\times192/1,128(\times2_{GPU})\)</span></td>
<td><span class="math">\(13\times13\times128(\times2_{GPU})\)</span></td>
<td><span class="math">\((3\times3\times192+1)\times128\times2\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max}\)</span></td>
<td><span class="math">\(13\times13\times128(\times2_{GPU})\)</span></td>
<td><span class="math">\(3\times3/2(\times2_{GPU})\)</span></td>
<td><span class="math">\(6\times6\times128(\times2_{GPU})\)</span></td>
<td>0</td>
</tr>
<tr>
<td>全连接层<span class="math">\(F_6\)</span> <span class="math">\(^*\)</span></td>
<td><span class="math">\(6\times6\times128\times2_{GPU}\)</span></td>
<td><span class="math">\(9216\times2048(\times2_{GPU})\)</span></td>
<td><span class="math">\(1\times1\times2048(\times2_{GPU})\)</span></td>
<td><span class="math">\((9216+1)\times2048\times2\)</span></td>
</tr>
<tr>
<td>全连接层<span class="math">\(F_7\)</span></td>
<td><span class="math">\(1\times1\times2048\times2_{GPU}\)</span></td>
<td><span class="math">\(4096\times2048(\times2_{GPU})\)</span></td>
<td><span class="math">\(1\times1\times2048(\times2_{GPU})\)</span></td>
<td><span class="math">\((4096+1)\times2048\times2\)</span></td>
</tr>
<tr>
<td>全连接层<span class="math">\(F_8\)</span></td>
<td><span class="math">\(1\times1\times2048\times2_{GPU}\)</span></td>
<td><span class="math">\(4096\times1000\)</span></td>
<td><span class="math">\(1\times1\times1000\)</span></td>
<td><span class="math">\((4096+1)\times1000\times2\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>卷积层<span class="math">\(C_1\)</span>输入为<span class="math">\(224\times224\times3\)</span>的图片数据，分别在两个GPU中经过核为<span class="math">\(11\times11\times3\)</span>、步长（stride）为4的卷积卷积后，分别得到两条独立的<span class="math">\(55\times55\times48\)</span>的输出数据。</p>
<p>下采样层<span class="math">\(S_{max}\)</span>实际上是嵌套在卷积中的最大池化操作，但是为了区分没有采用最大池化的卷积层单独列出来。在<span class="math">\(C_{1-2}\)</span>卷积层中的池化操作之后（ReLU激活操作之前），还有一个LRN操作，用作对相邻特征点的归一化处理。</p>
<p>卷积层<span class="math">\(C_3\)</span> 的输入与其他卷积层不同，<span class="math">\(13\times13\times192\times2_{GPU}\)</span>表示汇聚了上一层网络在两个GPU上的输出结果作为输入，所以在进行卷积操作时通道上的卷积核维度为384。</p>
<p>全连接层<span class="math">\(F_{6-8}\)</span>中输入数据尺寸也和<span class="math">\(C_3\)</span>类似，都是融合了两个GPU流向的输出结果作为输入。</p>
</blockquote>
<h3 id="423">4.2.3 模型特性<a class="headerlink" href="#423" title="Permanent link">¶</a></h3>
<ul>
<li>所有卷积层都使用ReLU作为非线性映射函数，使模型收敛速度更快</li>
<li>在多个GPU上进行模型的训练，不但可以提高模型的训练速度，还能提升数据的使用规模</li>
<li>使用LRN对局部的特征进行归一化，结果作为ReLU激活函数的输入能有效降低错误率</li>
<li>重叠最大池化（overlapping max pooling），即池化范围z与步长s存在关系<span class="math">\(z&gt;s\)</span>（如<span class="math">\(S_{max}\)</span>中核尺度为<span class="math">\(3\times3/2\)</span>），避免平均池化（average pooling）的平均效应</li>
<li>使用随机丢弃技术（dropout）选择性地忽略训练中的单个神经元，避免模型的过拟合</li>
</ul>
<h2 id="43-zfnet">4.3 ZFNet<a class="headerlink" href="#43-zfnet" title="Permanent link">¶</a></h2>
<h3 id="431">4.3.1 模型介绍<a class="headerlink" href="#431" title="Permanent link">¶</a></h3>
<p>​   ZFNet是由<span class="math">\(Matthew\)</span> <span class="math">\(D. Zeiler\)</span>和<span class="math">\(Rob\)</span> <span class="math">\(Fergus\)</span>在AlexNet基础上提出的大型卷积网络，在2013年ILSVRC图像分类竞赛中以11.19%的错误率获得冠军（实际上原ZFNet所在的队伍并不是真正的冠军，原ZFNet以13.51%错误率排在第8，真正的冠军是<span class="math">\(Clarifai\)</span>这个队伍，而<span class="math">\(Clarifai\)</span>这个队伍所对应的一家初创公司的CEO又是<span class="math">\(Zeiler\)</span>，而且<span class="math">\(Clarifai\)</span>对ZFNet的改动比较小，所以通常认为是ZFNet获得了冠军）<span class="math">\(^{[3-4]}​\)</span>。ZFNet实际上是微调（fine-tuning）了的AlexNet，并通过反卷积（Deconvolution）的方式可视化各层的输出特征图，进一步解释了卷积操作在大型网络中效果显著的原因。</p>
<h3 id="432">4.3.2 模型结构<a class="headerlink" href="#432" title="Permanent link">¶</a></h3>
<p><img alt="" src="./img/ch4/image21.png"/></p>
<p><img alt="" src="./img/ch4/image21.jpeg"/></p>
<p>​                       图4.4 ZFNet网络结构图（原始结构图与AlexNet风格结构图）</p>
<p>​   如图4.4所示，ZFNet与AlexNet类似，都是由8层网络组成的卷积神经网络，其中包含5层卷积层和3层全连接层。两个网络结构最大的不同在于，ZFNet第一层卷积采用了<span class="math">\(7\times7\times3/2\)</span>的卷积核替代了AlexNet中第一层卷积核<span class="math">\(11\times11\times3/4\)</span>的卷积核。图4.5中ZFNet相比于AlexNet在第一层输出的特征图中包含更多中间频率的信息，而AlexNet第一层输出的特征图大多是低频或高频的信息，对中间频率特征的缺失导致后续网络层次如图4.5（c）能够学习到的特征不够细致，而导致这个问题的根本原因在于AlexNet在第一层中采用的卷积核和步长过大。</p>
<p><img alt="" src="./img/ch4/zfnet-layer1.png"/></p>
<p><img alt="" src="./img/ch4/zfnet-layer2.png"/></p>
<p>​   图4.5 （a）ZFNet第一层输出的特征图（b）AlexNet第一层输出的特征图（c）AlexNet第二层输出的特征图（d）ZFNet第二层输出的特征图</p>
<p>​                                   表4.3 ZFNet网络参数配置
|        网络层         |               输入尺寸               |                  核尺寸                  |               输出尺寸               |              可训练参数量               |
| :—————————-: | :—————————————————: | :———————————————————: | :—————————————————: | :——————————————————-: |
|   卷积层<span class="math">\(C_1\)</span> <span class="math">\(^*\)</span>  |        <span class="math">\(224\times224\times3\)</span>         |          <span class="math">\(7\times7\times3/2,96\)</span>          |        <span class="math">\(110\times110\times96\)</span>        |      <span class="math">\((7\times7\times3+1)\times96\)</span>      |
| 下采样层<span class="math">\(S_{max}\)</span> |        <span class="math">\(110\times110\times96\)</span>        |               <span class="math">\(3\times3/2\)</span>               |         <span class="math">\(55\times55\times96\)</span>         |                    0                    |
|      卷积层<span class="math">\(C_2\)</span> <span class="math">\(^*\)</span>      |         <span class="math">\(55\times55\times96\)</span>         |         <span class="math">\(5\times5\times96/2,256\)</span>         |        <span class="math">\(26\times26\times256\)</span>        | <span class="math">\((5\times5\times96+1)\times256\)</span> |
|   下采样层<span class="math">\(S_{max}\)</span>   | <span class="math">\(26\times26\times256\)</span> |       <span class="math">\(3\times3/2\)</span>       | <span class="math">\(13\times13\times256\)</span> |                    0                    |
|   卷积层<span class="math">\(C_3\)</span>  |  <span class="math">\(13\times13\times256\)</span>  | <span class="math">\(3\times3\times256/1,384\)</span> | <span class="math">\(13\times13\times384\)</span> | <span class="math">\((3\times3\times256+1)\times384\)</span> |
|      卷积层<span class="math">\(C_4\)</span>      | <span class="math">\(13\times13\times384\)</span> | <span class="math">\(3\times3\times384/1,384\)</span> | <span class="math">\(13\times13\times384\)</span> | <span class="math">\((3\times3\times384+1)\times384\)</span> |
|      卷积层<span class="math">\(C_5\)</span>      | <span class="math">\(13\times13\times384\)</span> | <span class="math">\(3\times3\times384/1,256\)</span> | <span class="math">\(13\times13\times256\)</span> | <span class="math">\((3\times3\times384+1)\times256\)</span> |
|   下采样层<span class="math">\(S_{max}\)</span>   | <span class="math">\(13\times13\times256\)</span> |       <span class="math">\(3\times3/2\)</span>       |  <span class="math">\(6\times6\times256\)</span>  |                    0                    |
|  全连接层<span class="math">\(F_6\)</span>  |   <span class="math">\(6\times6\times256\)</span>   |     <span class="math">\(9216\times4096\)</span>     | <span class="math">\(1\times1\times4096\)</span> |       <span class="math">\((9216+1)\times4096\)</span>       |
|     全连接层<span class="math">\(F_7\)</span>     |  <span class="math">\(1\times1\times4096\)</span>  |     <span class="math">\(4096\times4096\)</span>     | <span class="math">\(1\times1\times4096\)</span> |       <span class="math">\((4096+1)\times4096\)</span>       |
|     全连接层<span class="math">\(F_8\)</span>     | <span class="math">\(1\times1\times4096\)</span> |             <span class="math">\(4096\times1000\)</span>             |         <span class="math">\(1\times1\times1000\)</span>         |       <span class="math">\((4096+1)\times1000\)</span> |</p>
<blockquote>
<p>卷积层<span class="math">\(C_1\)</span>与AlexNet中的<span class="math">\(C_1\)</span>有所不同，采用<span class="math">\(7\times7\times3/2\)</span>的卷积核代替<span class="math">\(11\times11\times3/4​\)</span>，使第一层卷积输出的结果可以包含更多的中频率特征，对后续网络层中多样化的特征组合提供更多选择，有利于捕捉更细致的特征。</p>
<p>卷积层<span class="math">\(C_2\)</span>采用了步长2的卷积核，区别于AlexNet中<span class="math">\(C_2\)</span>的卷积核步长，所以输出的维度有所差异。</p>
</blockquote>
<h3 id="433">4.3.3 模型特性<a class="headerlink" href="#433" title="Permanent link">¶</a></h3>
<p>​ ZFNet与AlexNet在结构上几乎相同，此部分虽属于模型特性，但准确地说应该是ZFNet原论文中可视化技术的贡献。</p>
<ul>
<li>可视化技术揭露了激发模型中每层单独的特征图。</li>
<li>可视化技术允许观察在训练阶段特征的演变过程且诊断出模型的潜在问题。</li>
<li>可视化技术用到了多层解卷积网络，即由特征激活返回到输入像素空间。</li>
<li>可视化技术进行了分类器输出的敏感性分析，即通过阻止部分输入图像来揭示那部分对于分类是重要的。</li>
<li>可视化技术提供了一个非参数的不变性来展示来自训练集的哪一块激活哪个特征图，不仅需要裁剪输入图片，而且自上而下的投影来揭露来自每块的结构激活一个特征图。</li>
<li>可视化技术依赖于解卷积操作，即卷积操作的逆过程，将特征映射到像素上。</li>
</ul>
<h2 id="44-network-in-network">4.4 Network in Network<a class="headerlink" href="#44-network-in-network" title="Permanent link">¶</a></h2>
<h3 id="441">4.4.1 模型介绍<a class="headerlink" href="#441" title="Permanent link">¶</a></h3>
<p>​   Network In Network (<span class="caps">NIN</span>)是由<span class="math">\(Min Lin\)</span>等人提出，在CIFAR-10和CIFAR-100分类任务中达到当时的最好水平，因其网络结构是由三个多层感知机堆叠而被成为NIN<span class="math">\(^{[5]}\)</span>。NIN以一种全新的角度审视了卷积神经网络中的卷积核设计，通过引入子网络结构代替纯卷积中的线性映射部分，这种形式的网络结构激发了更复杂的卷积神经网络的结构设计，其中下一节中介绍的GoogLeNet的Inception结构就是来源于这个思想。</p>
<h3 id="442">4.4.2 模型结构<a class="headerlink" href="#442" title="Permanent link">¶</a></h3>
<p><img alt="" src="./img/ch4/image23.png"/>
​                                   图 4.6 NIN网络结构图</p>
<p>​   NIN由三层的多层感知卷积层（MLPConv Layer）构成，每一层多层感知卷积层内部由若干层的局部全连接层和非线性激活函数组成，代替了传统卷积层中采用的线性卷积核。在网络推理（inference）时，这个多层感知器会对输入特征图的局部特征进行划窗计算，并且每个划窗的局部特征图对应的乘积的权重是共享的，这两点是和传统卷积操作完全一致的，最大的不同在于多层感知器对局部特征进行了非线性的映射，而传统卷积的方式是线性的。NIN的网络参数配置表4.4所示（原论文并未给出网络参数，表中参数为编者结合网络结构图和CIFAR-100数据集以<span class="math">\(3\times3\)</span>卷积为例给出）。</p>
<p>​                   表4.4 NIN网络参数配置（结合原论文NIN结构和CIFAR-100数据给出）</p>
<table>
<thead>
<tr>
<th align="center">网络层</th>
<th align="center">输入尺寸</th>
<th align="center">核尺寸</th>
<th align="center">输出尺寸</th>
<th align="center">参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">局部全连接层<span class="math">\(L_{11}\)</span> <span class="math">\(^*\)</span></td>
<td align="center"><span class="math">\(32\times32\times3\)</span></td>
<td align="center"><span class="math">\((3\times3)\times16/1\)</span></td>
<td align="center"><span class="math">\(30\times30\times16\)</span></td>
<td align="center"><span class="math">\((3\times3\times3+1)\times16\)</span></td>
</tr>
<tr>
<td align="center">全连接层<span class="math">\(L_{12}\)</span> <span class="math">\(^*\)</span></td>
<td align="center"><span class="math">\(30\times30\times16\)</span></td>
<td align="center"><span class="math">\(16\times16\)</span></td>
<td align="center"><span class="math">\(30\times30\times16\)</span></td>
<td align="center"><span class="math">\(((16+1)\times16)\)</span></td>
</tr>
<tr>
<td align="center">局部全连接层<span class="math">\(L_{21}\)</span></td>
<td align="center"><span class="math">\(30\times30\times16\)</span></td>
<td align="center"><span class="math">\((3\times3)\times64/1\)</span></td>
<td align="center"><span class="math">\(28\times28\times64\)</span></td>
<td align="center"><span class="math">\((3\times3\times16+1)\times64\)</span></td>
</tr>
<tr>
<td align="center">全连接层<span class="math">\(L_{22}\)</span></td>
<td align="center"><span class="math">\(28\times28\times64\)</span></td>
<td align="center"><span class="math">\(64\times64\)</span></td>
<td align="center"><span class="math">\(28\times28\times64\)</span></td>
<td align="center"><span class="math">\(((64+1)\times64)\)</span></td>
</tr>
<tr>
<td align="center">局部全连接层<span class="math">\(L_{31}\)</span></td>
<td align="center"><span class="math">\(28\times28\times64\)</span></td>
<td align="center"><span class="math">\((3\times3)\times100/1\)</span></td>
<td align="center"><span class="math">\(26\times26\times100\)</span></td>
<td align="center"><span class="math">\((3\times3\times64+1)\times100\)</span></td>
</tr>
<tr>
<td align="center">全连接层<span class="math">\(L_{32}\)</span></td>
<td align="center"><span class="math">\(26\times26\times100\)</span></td>
<td align="center"><span class="math">\(100\times100\)</span></td>
<td align="center"><span class="math">\(26\times26\times100\)</span></td>
<td align="center"><span class="math">\(((100+1)\times100)\)</span></td>
</tr>
<tr>
<td align="center">全局平均采样<span class="math">\(GAP\)</span> <span class="math">\(^*\)</span></td>
<td align="center"><span class="math">\(26\times26\times100\)</span></td>
<td align="center"><span class="math">\(26\times26\times100/1\)</span></td>
<td align="center"><span class="math">\(1\times1\times100\)</span></td>
<td align="center"><span class="math">\(0\)</span></td>
</tr>
<tr>
<td align="center">&gt; 局部全连接层<span class="math">\(L_{11}\)</span>实际上是对原始输入图像进行划窗式的全连接操作，因此划窗得到的输出特征尺寸为<span class="math">\(30\times30\)</span>（<span class="math">\(\frac{32-3_k+1}{1_{stride}}=30\)</span>）</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">&gt; 全连接层<span class="math">\(L_{12}\)</span>是紧跟<span class="math">\(L_{11}\)</span>后的全连接操作，输入的特征是划窗后经过激活的局部响应特征，因此仅需连接<span class="math">\(L_{11}\)</span>和<span class="math">\(L_{12}\)</span>的节点即可，而每个局部全连接层和紧接的全连接层构成代替卷积操作的多层感知卷积层（MLPConv）。</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">&gt; 全局平均采样层或全局平均池化层<span class="math">\(GAP\)</span>（Global Average Pooling）将<span class="math">\(L_{32}\)</span>输出的每一个特征图进行全局的平均池化操作，直接得到最后的类别数，可以有效地减少参数量。</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h3 id="443">4.4.3 模型特点<a class="headerlink" href="#443" title="Permanent link">¶</a></h3>
<ul>
<li>使用多层感知机结构来代替卷积的滤波操作，不但有效减少卷积核数过多而导致的参数量暴涨问题，还能通过引入非线性的映射来提高模型对特征的抽象能力。</li>
<li>使用全局平均池化来代替最后一个全连接层，能够有效地减少参数量（没有可训练参数），同时池化用到了整个特征图的信息，对空间信息的转换更加鲁棒，最后得到的输出结果可直接作为对应类别的置信度。</li>
</ul>
<h2 id="45-vggnet">4.5 VGGNet<a class="headerlink" href="#45-vggnet" title="Permanent link">¶</a></h2>
<h3 id="451">4.5.1 模型介绍<a class="headerlink" href="#451" title="Permanent link">¶</a></h3>
<p>​   VGGNet是由牛津大学视觉几何小组（Visual Geometry Group, <span class="caps">VGG</span>）提出的一种深层卷积网络结构，他们以7.32%的错误率赢得了2014年ILSVRC分类任务的亚军（冠军由GoogLeNet以6.65%的错误率夺得）和25.32%的错误率夺得定位任务（Localization）的第一名（GoogLeNet错误率为26.44%）<span class="math">\(^{[5]}\)</span>，网络名称VGGNet取自该小组名缩写。VGGNet是首批把图像分类的错误率降低到10%以内模型，同时该网络所采用的<span class="math">\(3\times3\)</span>卷积核的思想是后来许多模型的基础，该模型发表在2015年国际学习表征会议（International Conference On Learning Representations, <span class="caps">ICLR</span>）后至今被引用的次数已经超过1万4千余次。</p>
<h3 id="452">4.5.2 模型结构<a class="headerlink" href="#452" title="Permanent link">¶</a></h3>
<p><img alt="" src="./img/ch4/vgg16.png"/></p>
<p>​                               图 4.7 VGG16网络结构图</p>
<p>​   在原论文中的VGGNet包含了6个版本的演进，分别对应VGG11、<span class="caps">VGG11</span>-<span class="caps">LRN</span>、<span class="caps">VGG13</span>、<span class="caps">VGG16</span>-1、<span class="caps">VGG16</span>-3和VGG19，不同的后缀数值表示不同的网络层数（<span class="caps">VGG11</span>-LRN表示在第一层中采用了LRN的VGG11，<span class="caps">VGG16</span>-1表示后三组卷积块中最后一层卷积采用卷积核尺寸为<span class="math">\(1\times1\)</span>，相应的VGG16-3表示卷积核尺寸为<span class="math">\(3\times3\)</span>），本节介绍的VGG16为VGG16-3。图4.7中的VGG16体现了VGGNet的核心思路，使用<span class="math">\(3\times3\)</span>的卷积组合代替大尺寸的卷积（2个<span class="math">\(3\times3卷积即可与\)</span><span class="math">\(5\times5\)</span>卷积拥有相同的感受视野），网络参数设置如表4.5所示。</p>
<p>​                               表4.5 VGG16网络参数配置</p>
<table>
<thead>
<tr>
<th>网络层</th>
<th>输入尺寸</th>
<th>核尺寸</th>
<th>输出尺寸</th>
<th>参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td>卷积层<span class="math">\(C_{11}\)</span></td>
<td><span class="math">\(224\times224\times3\)</span></td>
<td><span class="math">\(3\times3\times64/1\)</span></td>
<td><span class="math">\(224\times224\times64\)</span></td>
<td><span class="math">\((3\times3\times3+1)\times64\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{12}\)</span></td>
<td><span class="math">\(224\times224\times64\)</span></td>
<td><span class="math">\(3\times3\times64/1\)</span></td>
<td><span class="math">\(224\times224\times64\)</span></td>
<td><span class="math">\((3\times3\times64+1)\times64\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max1}\)</span></td>
<td><span class="math">\(224\times224\times64\)</span></td>
<td><span class="math">\(2\times2/2\)</span></td>
<td><span class="math">\(112\times112\times64\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{21}\)</span></td>
<td><span class="math">\(112\times112\times64\)</span></td>
<td><span class="math">\(3\times3\times128/1\)</span></td>
<td><span class="math">\(112\times112\times128\)</span></td>
<td><span class="math">\((3\times3\times64+1)\times128\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{22}\)</span></td>
<td><span class="math">\(112\times112\times128\)</span></td>
<td><span class="math">\(3\times3\times128/1\)</span></td>
<td><span class="math">\(112\times112\times128\)</span></td>
<td><span class="math">\((3\times3\times128+1)\times128\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max2}\)</span></td>
<td><span class="math">\(112\times112\times128\)</span></td>
<td><span class="math">\(2\times2/2\)</span></td>
<td><span class="math">\(56\times56\times128\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{31}\)</span></td>
<td><span class="math">\(56\times56\times128\)</span></td>
<td><span class="math">\(3\times3\times256/1\)</span></td>
<td><span class="math">\(56\times56\times256\)</span></td>
<td><span class="math">\((3\times3\times128+1)\times256\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{32}\)</span></td>
<td><span class="math">\(56\times56\times256\)</span></td>
<td><span class="math">\(3\times3\times256/1\)</span></td>
<td><span class="math">\(56\times56\times256\)</span></td>
<td><span class="math">\((3\times3\times256+1)\times256\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{33}\)</span></td>
<td><span class="math">\(56\times56\times256\)</span></td>
<td><span class="math">\(3\times3\times256/1\)</span></td>
<td><span class="math">\(56\times56\times256\)</span></td>
<td><span class="math">\((3\times3\times256+1)\times256\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max3}\)</span></td>
<td><span class="math">\(56\times56\times256\)</span></td>
<td><span class="math">\(2\times2/2\)</span></td>
<td><span class="math">\(28\times28\times256\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{41}\)</span></td>
<td><span class="math">\(28\times28\times256\)</span></td>
<td><span class="math">\(3\times3\times512/1\)</span></td>
<td><span class="math">\(28\times28\times512\)</span></td>
<td><span class="math">\((3\times3\times256+1)\times512\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{42}\)</span></td>
<td><span class="math">\(28\times28\times512\)</span></td>
<td><span class="math">\(3\times3\times512/1\)</span></td>
<td><span class="math">\(28\times28\times512\)</span></td>
<td><span class="math">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{43}\)</span></td>
<td><span class="math">\(28\times28\times512\)</span></td>
<td><span class="math">\(3\times3\times512/1\)</span></td>
<td><span class="math">\(28\times28\times512\)</span></td>
<td><span class="math">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max4}\)</span></td>
<td><span class="math">\(28\times28\times512\)</span></td>
<td><span class="math">\(2\times2/2\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{51}\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\(3\times3\times512/1\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{52}\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\(3\times3\times512/1\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{53}\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\(3\times3\times512/1\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\((3\times3\times512+1)\times512\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{max5}\)</span></td>
<td><span class="math">\(14\times14\times512\)</span></td>
<td><span class="math">\(2\times2/2\)</span></td>
<td><span class="math">\(7\times7\times512\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr>
<td>全连接层<span class="math">\(FC_{1}\)</span></td>
<td><span class="math">\(7\times7\times512\)</span></td>
<td><span class="math">\((7\times7\times512)\times4096\)</span></td>
<td><span class="math">\(1\times4096\)</span></td>
<td><span class="math">\((7\times7\times512+1)\times4096\)</span></td>
</tr>
<tr>
<td>全连接层<span class="math">\(FC_{2}\)</span></td>
<td><span class="math">\(1\times4096\)</span></td>
<td><span class="math">\(4096\times4096\)</span></td>
<td><span class="math">\(1\times4096\)</span></td>
<td><span class="math">\((4096+1)\times4096\)</span></td>
</tr>
<tr>
<td>全连接层<span class="math">\(FC_{3}\)</span></td>
<td><span class="math">\(1\times4096\)</span></td>
<td><span class="math">\(4096\times1000\)</span></td>
<td><span class="math">\(1\times1000\)</span></td>
<td><span class="math">\((4096+1)\times1000\)</span></td>
</tr>
</tbody>
</table>
<h3 id="453">4.5.3 模型特性<a class="headerlink" href="#453" title="Permanent link">¶</a></h3>
<ul>
<li>整个网络都使用了同样大小的卷积核尺寸<span class="math">\(3\times3\)</span>和最大池化尺寸<span class="math">\(2\times2\)</span>。</li>
<li><span class="math">\(1\times1\)</span>卷积的意义主要在于线性变换，而输入通道数和输出通道数不变，没有发生降维。</li>
<li>两个<span class="math">\(3\times3\)</span>的卷积层串联相当于1个<span class="math">\(5\times5\)</span>的卷积层，感受野大小为<span class="math">\(5\times5\)</span>。同样地，3个<span class="math">\(3\times3\)</span>的卷积层串联的效果则相当于1个<span class="math">\(7\times7\)</span>的卷积层。这样的连接方式使得网络参数量更小，而且多层的激活函数令网络对特征的学习能力更强。</li>
<li>VGGNet在训练时有一个小技巧，先训练浅层的的简单网络VGG11，再复用VGG11的权重来初始化VGG13，如此反复训练并初始化VGG19，能够使训练时收敛的速度更快。</li>
<li>在训练过程中使用多尺度的变换对原始数据做数据增强，使得模型不易过拟合。</li>
</ul>
<h2 id="46-googlenet">4.6 GoogLeNet<a class="headerlink" href="#46-googlenet" title="Permanent link">¶</a></h2>
<h3 id="461">4.6.1 模型介绍<a class="headerlink" href="#461" title="Permanent link">¶</a></h3>
<p>​   GoogLeNet作为2014年ILSVRC在分类任务上的冠军，以6.65%的错误率力压VGGNet等模型，在分类的准确率上面相比过去两届冠军ZFNet和AlexNet都有很大的提升。从名字<strong>GoogLe</strong>Net可以知道这是来自谷歌工程师所设计的网络结构，而名字中Goog<strong>LeNet</strong>更是致敬了LeNet<span class="math">\(^{[0]}\)</span>。GoogLeNet中最核心的部分是其内部子网络结构Inception，该结构灵感来源于NIN，至今已经经历了四次版本迭代（Inception<span class="math">\(_{v1-4}\)</span>）。</p>
<p><img alt="" src="./img/ch4/img_inception_01.png"/>
​                   图 4.8 Inception性能比较图</p>
<h3 id="462">4.6.2 模型结构<a class="headerlink" href="#462" title="Permanent link">¶</a></h3>
<p><img alt="" src="./img/ch4/image25.jpeg"/>
​                   图 4.9 GoogLeNet网络结构图
​   如图4.9中所示，GoogLeNet相比于以前的卷积神经网络结构，除了在深度上进行了延伸，还对网络的宽度进行了扩展，整个网络由许多块状子网络的堆叠而成，这个子网络构成了Inception结构。图4.9为Inception的四个版本：<span class="math">\(Inception_{v1}​\)</span>在同一层中采用不同的卷积核，并对卷积结果进行合并;<span class="math">\(Inception_{v2}​\)</span>组合不同卷积核的堆叠形式，并对卷积结果进行合并;<span class="math">\(Inception_{v3}​\)</span>则在<span class="math">\(v_2​\)</span>基础上进行深度组合的尝试;<span class="math">\(Inception_{v4}​\)</span>结构相比于前面的版本更加复杂，子网络中嵌套着子网络。</p>
<p><span class="math">\(Inception_{v1}\)</span></p>
<p><img alt="" src="./img/ch4/image27.png"/></p>
<p><img alt="" src="./img/ch4/image28.png"/></p>
<p><span class="math">\(Inception_{v2}\)</span></p>
<p><img alt="" src="./img/ch4/image34.png"/></p>
<p><img alt="" src="./img/ch4/image36.png"/></p>
<p><img alt="" src="./img/ch4/image38.png"/></p>
<p><span class="math">\(Inception_{v3}\)</span></p>
<p><img alt="" src="./img/ch4/image37.png"/></p>
<p><span class="math">\(Inception_{v4}\)</span></p>
<p><img alt="" src="./img/ch4/image46.png"/></p>
<p><img alt="" src="./img/ch4/image47.png"/></p>
<p><img alt="" src="./img/ch4/image63.png"/></p>
<p>​                   图 4.10 Inception<span class="math">\(_{v1-4}\)</span>结构图</p>
<p>​                   表 4.6 GoogLeNet中Inception<span class="math">\(_{v1}\)</span>网络参数配置</p>
<table>
<thead>
<tr>
<th>网络层</th>
<th>输入尺寸</th>
<th>核尺寸</th>
<th>输出尺寸</th>
<th>参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td>卷积层<span class="math">\(C_{11}\)</span></td>
<td><span class="math">\(H\times{W}\times{C_1}\)</span></td>
<td><span class="math">\(1\times1\times{C_2}/2\)</span></td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times{C_2}\)</span></td>
<td><span class="math">\((1\times1\times{C_1}+1)\times{C_2}\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{21}\)</span></td>
<td><span class="math">\(H\times{W}\times{C_2}\)</span></td>
<td><span class="math">\(1\times1\times{C_2}/2\)</span></td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times{C_2}\)</span></td>
<td><span class="math">\((1\times1\times{C_2}+1)\times{C_2}\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{22}\)</span></td>
<td><span class="math">\(H\times{W}\times{C_2}\)</span></td>
<td><span class="math">\(3\times3\times{C_2}/1\)</span></td>
<td><span class="math">\(H\times{W}\times{C_2}/1\)</span></td>
<td><span class="math">\((3\times3\times{C_2}+1)\times{C_2}\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{31}\)</span></td>
<td><span class="math">\(H\times{W}\times{C_1}\)</span></td>
<td><span class="math">\(1\times1\times{C_2}/2\)</span></td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times{C_2}\)</span></td>
<td><span class="math">\((1\times1\times{C_1}+1)\times{C_2}\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{32}\)</span></td>
<td><span class="math">\(H\times{W}\times{C_2}\)</span></td>
<td><span class="math">\(5\times5\times{C_2}/1\)</span></td>
<td><span class="math">\(H\times{W}\times{C_2}/1\)</span></td>
<td><span class="math">\((5\times5\times{C_2}+1)\times{C_2}\)</span></td>
</tr>
<tr>
<td>下采样层<span class="math">\(S_{41}\)</span></td>
<td><span class="math">\(H\times{W}\times{C_1}\)</span></td>
<td><span class="math">\(3\times3/2\)</span></td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times{C_2}\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr>
<td>卷积层<span class="math">\(C_{42}\)</span></td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times{C_2}\)</span></td>
<td><span class="math">\(1\times1\times{C_2}/1\)</span></td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times{C_2}\)</span></td>
<td><span class="math">\((3\times3\times{C_2}+1)\times{C_2}\)</span></td>
</tr>
<tr>
<td>合并层<span class="math">\(M\)</span></td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times{C_2}(\times4)\)</span></td>
<td>拼接</td>
<td><span class="math">\(\frac{H}{2}\times\frac{W}{2}\times({C_2}\times4)\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
</tbody>
</table>
<h3 id="463">4.6.3 模型特性<a class="headerlink" href="#463" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； </p>
</li>
<li>
<p>之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；</p>
</li>
<li>
<p>网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN2，采用1x1卷积核来进行降维。</p>
</li>
</ul>
<p># </p>
<h2 id="restnet">Restnet<a class="headerlink" href="#restnet" title="Permanent link">¶</a></h2>
<h2 id="densenet">Densenet<a class="headerlink" href="#densenet" title="Permanent link">¶</a></h2>
<h2 id="47-cnngooglenetvggnetalexnet">4.7 为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？<a class="headerlink" href="#47-cnngooglenetvggnetalexnet" title="Permanent link">¶</a></h2>
<ul>
<li>评测对比：为了让自己的结果更有说服力，在发表自己成果的时候会同一个标准的baseline及在baseline上改进而进行比较，常见的比如各种检测分割的问题都会基于VGG或者Resnet101这样的基础网络。</li>
<li>时间和精力有限：在科研压力和工作压力中，时间和精力只允许大家在有限的范围探索。</li>
<li>模型创新难度大：进行基本模型的改进需要大量的实验和尝试，并且需要大量的实验积累和强大灵感，很有可能投入产出比比较小。</li>
<li>资源限制：创造一个新的模型需要大量的时间和计算资源，往往在学校和小型商业团队不可行。</li>
<li>在实际的应用场景中，其实是有大量的非标准模型的配置。</li>
</ul>
<h2 id="_2">参考文献<a class="headerlink" href="#_2" title="Permanent link">¶</a></h2>
<p>[1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. <em>Proceedings of the <span class="caps">IEEE</span></em>, november 1998.</p>
<p>[2] A. Krizhevsky, I. Sutskever and <span class="caps">G. E.</span> Hinton. ImageNet Classification with Deep Convolutional Neural Networks. <em>Advances in Neural Information Processing Systems 25</em>. Curran Associates, Inc. 1097–1105.</p>
<p>[3] <span class="caps">LSVRC</span>-2013. http://www.image-net.org/challenges/<span class="caps">LSVRC</span>/2013/results.php</p>
<p>[4] <span class="caps">M. D.</span> Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks. <em>European Conference on Computer Vision</em>. </p>
<p>[5] M. Lin,  Q. Chen,  and S. Yan.   Network in network. <em>Computing Research Repository</em>, abs/1312.4400, 2013.</p>
<p>[6] K. Simonyan and A. Zisserman.  Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>International Conference on Machine Learning</em>, 2015.</p>
<p>[7] Bharath Raj. <a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">a-simple-guide-to-the-versions-of-the-inception-network</a>, 2018.</p>
<p>[8] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. <a href="https://arxiv.org/pdf/1602.07261.pdf">Inception-v4, Inception-ResNet and
the Impact of Residual Connections on Learning</a>, 2016.</p>
<p>[9] Sik-Ho Tsang. <a href="https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification</a>, 2018.</p>
<p>[10] Zbigniew Wojna, Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens. <a href="https://arxiv.org/pdf/1512.00567v3.pdf">Rethinking the Inception Architecture for Computer Vision</a>, 2015.</p>
<p>[11] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. <a href="https://arxiv.org/pdf/1409.4842v1.pdf">Going deeper with convolutions</a>, 2014.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            
            <section>
<div class="accordion" id="accordion2">
    <div class="accordion-group">
        <div class="accordion-heading">
            <a class="accordion-toggle disqus-comment-count" data-toggle="collapse" data-parent="#accordion2"
                href="https://csbwang.github.io/dl_ch4#disqus_thread">
                Comments
            </a>
        </div>
        <div id="disqus_thread" class="accordion-body collapse">
            <div class="accordion-inner">
                <div class="comments">
                    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'FelixWang';
        var disqus_identifier = 'https://csbwang.github.io/dl_ch4';
    var disqus_url = 'https://csbwang.github.io/dl_ch4';

    (function() {
         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

                </div>
            </div>
        </div>
    </div>
</div>
</section>

            <hr/>
<section>
    <h2>Keep Reading</h2>
<ul class="related-posts-list">
<li><a href="https://csbwang.github.io/dl_ch3" title="第三章 深度学习基础">第三章 深度学习基础</a></li>
<li><a href="https://csbwang.github.io/dl_ch18_1" title="第十八章_后端架构选型、离线及实时计算">第十八章_后端架构选型、离线及实时计算</a></li>
<li><a href="https://csbwang.github.io/dl_ch11" title="第十一章 迁移学习">第十一章 迁移学习</a></li>
<li><a href="https://csbwang.github.io/dl_ch8" title="第八章 目标检测">第八章 目标检测</a></li>
<li><a href="https://csbwang.github.io/dl_ch13" title="第一十三章 优化算法">第一十三章 优化算法</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="https://csbwang.github.io/dl_ch5" title="Previous: 第五章 卷积神经网络（CNN）">第五章 卷积神经网络（CNN）</a></li>
                <li class="next-article"><a href="https://csbwang.github.io/dl_ch3" title="Next: 第三章 深度学习基础">第三章 深度学习基础</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2019-07-06T00:00:00+08:00"> 7 6, 2019</time>

<h4>Last Updated</h4>
<time datetime="2019-07-06T00:00:00+08:00"> 7 6, 2019</time>

            <h4>Category</h4>
            <a class="category-link" href="https://csbwang.github.io/categories.html#shen-du-xue-xi-500wen-ref">深度学习500问</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://csbwang.github.io/tags.html#machine-learning-ref">machine learning
                    <span>20</span>
</a></li>
            </ul>
<h4>Stay in Touch</h4>
    <a href="https://github.com/csbwang" title="My Github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
<!-- Begin MailChimp Signup Form -->
<div id="mc-embed-signup">
<form action="empty" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<h4>Get Monthly Updates</h4>
<input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="Enter your email..." required>
<div class="clear"><input type="submit" value="Send me Free updates" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
</form>
</div>
<!--End mc_embed_signup-->
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="https://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

            <script type="text/javascript">
var disqus_shortname = 'FelixWang';
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
<script  language="javascript" type="text/javascript">
function uncollapse() {
    if (window.location.hash.match(/^#comment-\d+$/)) {
        $('#disqus_thread').collapse('show');
    }
}
</script>
<script type="text/javascript" language="JavaScript">
uncollapse();
window.onhashchange=function(){
    if (window.location.hash.match(/^#comment-\d+$/))
        window.location.reload(true);
}
</script>
<script>
$('#disqus_thread').on('shown', function () {
    var link = document.getElementsByClassName('accordion-toggle');
    var old_innerHTML = link[0].innerHTML;
    $(link[0]).fadeOut(500, function() {
        $(this).text('Click here to hide comments').fadeIn(500);
    });
    $('#disqus_thread').on('hidden', function () {
        $(link[0]).fadeOut(500, function() {
            $(this).text(old_innerHTML).fadeIn(500);
        });
    })
})
</script>


    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>